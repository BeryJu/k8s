---
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: snapshot-cronjob-controller
  annotations:
    policies.kyverno.io/title: Snapshot CronJob controller
    policies.kyverno.io/subject: PersistentVolumeClaim
    policies.kyverno.io/description: |
      This policy creates a Kopia snapshot CronJob for labeled PersistentVolumeClaims

      The following labels on PVCs with their respective labels are required for this to run:
        - snapshot.home.arpa/enabled
        - app.kubernetes.io/name
        - app.kubernetes.io/instance

      An optional label of "snapshot.home.arpa/ignoreAffinity" may be set on the PVC
      if the pod is guaranteed to not run during the time of this jobs execution
spec:
  generateExistingOnPolicyUpdate: true
  mutateExistingOnPolicyUpdate: true
  rules:
    - name: create-snapshot-cronjob-credential
      match:
        any:
          - resources:
              kinds:
                - PersistentVolumeClaim
              selector:
                matchLabels:
                  snapshot.home.arpa/enabled: "true"
                  app.kubernetes.io/name: "*"
                  app.kubernetes.io/instance: "*"
      context:
        - name: appName
          variable:
            jmesPath: 'request.object.metadata.labels."app.kubernetes.io/name"'
        - name: claimName
          variable:
            jmesPath: "request.object.metadata.name"
        - name: namespace
          variable:
            jmesPath: "request.object.metadata.namespace"
      generate:
        synchronize: true
        apiVersion: external-secrets.io/v1beta1
        kind: ExternalSecret
        name: "{{ appName }}-{{ claimName }}-snapshot-credentials"
        namespace: "{{ request.object.metadata.namespace }}"
        data:
          metadata:
            ownerReferences:
              - apiVersion: "{{ request.object.apiVersion }}"
                kind: "{{ request.object.kind }}"
                name: "{{ request.object.metadata.name }}"
                uid: "{{ request.object.metadata.uid }}"
          spec:
            secretStoreRef:
              name: vault-kv
              kind: ClusterSecretStore
            data:
              - secretKey: AWS_ACCESS_KEY_ID
                remoteRef:
                  key: kv/wasabi/backup
                  property: access_key
              - secretKey: AWS_SECRET_ACCESS_KEY
                remoteRef:
                  key: kv/wasabi/backup
                  property: secret_key
              - secretKey: KOPIA_PASSWORD
                remoteRef:
                  key: kv/wasabi/backup
                  property: kopia
    - name: create-snapshot-cronjob
      match:
        any:
          - resources:
              kinds:
                - PersistentVolumeClaim
              selector:
                matchLabels:
                  snapshot.home.arpa/enabled: "true"
                  app.kubernetes.io/name: "*"
                  app.kubernetes.io/instance: "*"
      context:
        - name: appName
          variable:
            jmesPath: 'request.object.metadata.labels."app.kubernetes.io/name"'
        - name: claimName
          variable:
            jmesPath: "request.object.metadata.name"
        - name: namespace
          variable:
            jmesPath: "request.object.metadata.namespace"
        - name: nodeAffinity
          variable:
            value:
              ignored: '{{ (request.object.metadata.labels."snapshot.home.arpa/ignoreAffinity" || ''false'') == ''false'' }}'
              labels:
                - key: app.kubernetes.io/name
                  operator: "In"
                  values:
                    - '{{ request.object.metadata.labels."app.kubernetes.io/name" }}'
                - key: app.kubernetes.io/instance
                  operator: "In"
                  values:
                    - '{{ request.object.metadata.labels."app.kubernetes.io/instance" }}'
      generate:
        synchronize: true
        apiVersion: batch/v1
        kind: CronJob
        name: "{{ appName }}-{{ claimName }}-snapshot"
        namespace: "{{ request.object.metadata.namespace }}"
        data:
          metadata:
            labels:
              app.kubernetes.io/name: '{{ request.object.metadata.labels."app.kubernetes.io/name" }}'
              app.kubernetes.io/instance: '{{ request.object.metadata.labels."app.kubernetes.io/instance" }}'
            ownerReferences:
              - apiVersion: "{{ request.object.apiVersion }}"
                kind: "{{ request.object.kind }}"
                name: "{{ request.object.metadata.name }}"
                uid: "{{ request.object.metadata.uid }}"
          spec:
            schedule: "0 3 * * *"
            suspend: false
            concurrencyPolicy: Forbid
            successfulJobsHistoryLimit: 1
            failedJobsHistoryLimit: 2
            jobTemplate:
              spec:
                # Keep at least one job in completed state in accordance to the schedule
                ttlSecondsAfterFinished: 86400
                template:
                  metadata:
                    annotations:
                      sidecar.istio.io/inject: "false"
                      linkerd.io/inject: "disabled"
                  spec:
                    automountServiceAccountToken: false
                    restartPolicy: OnFailure
                    # Stagger jobs to run randomly within X seconds to avoid bringing down all apps at once
                    initContainers:
                      - name: wait
                        image: ghcr.io/onedr0p/kopia:0.13.0
                        command: ["/scripts/sleep.sh"]
                        args: ["1", "900"]
                    containers:
                      - name: snapshot
                        image: ghcr.io/onedr0p/kopia:0.13.0
                        env:
                          - name: BUCKET
                            value: "beryjuio-k8s-backup"
                          - name: ENDPOINT
                            value: "s3.eu-central-2.wasabisys.com"
                        envFrom:
                          - secretRef:
                              name: "{{ appName }}-{{ claimName }}-snapshot-credentials"
                        command:
                          - /bin/bash
                          - -c
                          - |-
                            set -x
                            BACKUP_PATH="/data/{{ namespace }}/{{ appName }}/{{ claimName }}"
                            kopia repo connect s3 \
                              --endpoint="$${ENDPOINT}" \
                              --bucket="$${BUCKET}" \
                              --override-hostname="${CLUSTER_NAME}" \
                              --override-username="{{ request.object.metadata.namespace }}"
                            kopia policy set $${BACKUP_PATH} \
                              --compression=zstd \
                              --keep-latest 28 \
                              --keep-hourly 0 \
                              --keep-daily 7 \
                              --keep-weekly 2 \
                              --keep-monthly 2 \
                              --keep-annual 0
                            # fsfreeze -f $${BACKUP_PATH}
                            kopia snap create $${BACKUP_PATH}
                            # fsfreeze -u $${BACKUP_PATH}
                            kopia snap list $${BACKUP_PATH}
                            kopia content stats
                            kopia maintenance info
                            kopia repo disconnect
                        volumeMounts:
                          - name: data
                            mountPath: "/data/{{ namespace }}/{{ appName }}/{{ claimName }}"
                        securityContext:
                          privileged: true
                    volumes:
                      - name: data
                        persistentVolumeClaim:
                          claimName: "{{ claimName }}"
                    affinity:
                      podAffinity:
                        requiredDuringSchedulingIgnoredDuringExecution:
                          - topologyKey: kubernetes.io/hostname
                            labelSelector:
                              matchExpressions: "{{ nodeAffinity.ignored && [] || nodeAffinity.labels }}"
